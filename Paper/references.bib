
@online{owaspfoundationOWASPTopTen2024,
  title = {{{OWASP Top Ten}} | {{OWASP Foundation}}},
  author = {{OWASP Foundation}},
  date = {2024-11-02},
  url = {https://owasp.org/www-project-top-ten/},
  year = {2024},
  abstract = {The OWASP Top 10 is the reference standard for the most critical web application security risks. Adopting the OWASP Top 10 is perhaps the most effective first step towards changing your software development culture focused on producing secure code.},
  langid = {english},
  file = {/Users/parssajashnieh/Zotero/storage/UVR38LLF/www-project-top-ten.html}
}

@online{popescuEtalTowardsTheoryOfNaturalLanguage,
  title = {Towards a Theory of Natural Language Interfaces to Databases},
  author = {Ana-Maria Popescu, Oren Etzioni, Henry Kautz},
  date = {2003-01-12},
  url = {https://dl.acm.org/doi/10.1145/604045.604070},
  year = {2003},
  langid = {english}
}

@online{customerLeavingSites,
  title = {New research: 69\% of online shoppers go straight to the search bar when visiting ecommerce sites, but 80\% leave due to a poor experience},
  author = {Eve Rouse},
  date = {2023-02-15},
  url = {https://www.nosto.com/blog/new-search-research/},
  year = {2023},
  langid = {english}
}

@misc{androutsopoulosNaturalLanguageInterfaces1995,
	title = {Natural {Language} {Interfaces} to {Databases} - {An} {Introduction}},
	url = {http://arxiv.org/abs/cmp-lg/9503016},
	doi = {10.48550/arXiv.cmp-lg/9503016},
	abstract = {This paper is an introduction to natural language interfaces to databases (NLIDBs). A brief overview of the history of NLIDBs is first given. Some advantages and disadvantages of NLIDBs are then discussed, comparing NLIDBs to formal query languages, form-based interfaces, and graphical interfaces. An introduction to some of the linguistic problems NLIDBs have to confront follows, for the benefit of readers less familiar with computational linguistics. The discussion then moves on to NLIDB architectures, portability issues, restricted natural language input systems (including menu-based NLIDBs), and NLIDBs with reasoning capabilities. Some less explored areas of NLIDB research are then presented, namely database updates, meta-knowledge questions, temporal questions, and multi-modal NLIDBs. The paper ends with reflections on the current state of the art.},
	urldate = {2025-02-22},
	publisher = {arXiv},
	author = {Androutsopoulos, I. and Ritchie, G. D. and Thanisch, P.},
	month = mar,
	year = {1995},
	note = {arXiv:cmp-lg/9503016},
	keywords = {Computation and Language, Computer Science - Computation and Language},
	annote = {Comment: 50 pages, uuencoded compressed tar file, containing LaTeX code and .eps figures. Uses a4wide.sty. (No changes in the text. Fixed problem with epsf macro. Use the epsf.sty included in the tar file, not the epsf.sty of the cmp-lg server.)},
	file = {Preprint PDF:/Users/parssajashnieh/Zotero/storage/UAUER4RY/Androutsopoulos et al. - 1995 - Natural Language Interfaces to Databases - An Intr.pdf:application/pdf;Snapshot:/Users/parssajashnieh/Zotero/storage/LAMCV298/9503016.html:text/html},
}

@misc{manchandaIntentTermSelection2019,
	title = {Intent term selection and refinement in e-commerce queries},
	url = {http://arxiv.org/abs/1908.08564},
	doi = {10.48550/arXiv.1908.08564},
	abstract = {In e-commerce, a user tends to search for the desired product by issuing a query to the search engine and examining the retrieved results. If the search engine was successful in correctly understanding the user's query, it will return results that correspond to the products whose attributes match the terms in the query that are representative of the query's product intent. However, the search engine may fail to retrieve results that satisfy the query's product intent and thus degrading user experience due to different issues in query processing: (i) when multiple terms are present in a query it may fail to determine the relevant terms that are representative of the query's product intent, and (ii) it may suffer from vocabulary gap between the terms in the query and the product's description, i.e., terms used in the query are semantically similar but different from the terms in the product description. Hence, identifying the terms that describe the query's product intent and predicting additional terms that describe the query's product intent better than the existing query terms to the search engine is an essential task in e-commerce search. In this paper, we leverage the historical query reformulation logs of a major e-commerce retailer to develop distant-supervised approaches to solve both these problems. Our approaches exploit the fact that the significance of a term is dependent upon the context (other terms in the neighborhood) in which it is used in order to learn the importance of the term towards the query's product intent. We show that identifying and emphasizing the terms that define the query's product intent leads to a 3\% improvement in ranking. Moreover, for the tasks of identifying the important terms in a query and for predicting the additional terms that represent product intent, experiments illustrate that our approaches outperform the non-contextual baselines.},
	urldate = {2025-02-22},
	publisher = {arXiv},
	author = {Manchanda, Saurav and Sharma, Mohit and Karypis, George},
	month = aug,
	year = {2019},
	note = {arXiv:1908.08564 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	annote = {Comment: Extended version of paper "Intent term weighing in e-commerce queries" to appear in CIKM'19},
	file = {Preprint PDF:/Users/parssajashnieh/Zotero/storage/9JQ4D6GL/Manchanda et al. - 2019 - Intent term selection and refinement in e-commerce.pdf:application/pdf;Snapshot:/Users/parssajashnieh/Zotero/storage/2TQP2KZW/1908.html:text/html},
}

@inproceedings{greenBaseballAutomaticQuestionanswerer1961,
	address = {Los Angeles, California},
	title = {Baseball: an automatic question-answerer},
	shorttitle = {Baseball},
	url = {http://portal.acm.org/citation.cfm?doid=1460690.1460714},
	doi = {10.1145/1460690.1460714},
	language = {en},
	urldate = {2025-02-12},
	booktitle = {Papers presented at the {May} 9-11, 1961, western joint {IRE}-{AIEE}-{ACM} computer conference on - {IRE}-{AIEE}-{ACM} '61 ({Western})},
	publisher = {ACM Press},
	author = {Green, Bert F. and Wolf, Alice K. and Chomsky, Carol and Laughery, Kenneth},
	year = {1961},
	pages = {219},
	file = {Green et al. - 1961 - Baseball an automatic question-answerer.pdf:/Users/parssajashnieh/Zotero/storage/H3FPUHJC/Green et al. - 1961 - Baseball an automatic question-answerer.pdf:application/pdf},
}

@inproceedings{woods1973progress,
  title={Progress in natural language understanding: an application to lunar geology},
  author={Woods, William A},
  booktitle={Proceedings of the June 4-8, 1973, national computer conference and exposition},
  pages={441--450},
  year={1973},
  publisher={ACM}
}

@misc{raffelExploringLimitsTransfer2023,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	url = {http://arxiv.org/abs/1910.10683},
	doi = {10.48550/arXiv.1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	urldate = {2025-02-22},
	publisher = {arXiv},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = sep,
	year = {2023},
	note = {arXiv:1910.10683 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/parssajashnieh/Zotero/storage/ERQA54XT/Raffel et al. - 2023 - Exploring the Limits of Transfer Learning with a U.pdf:application/pdf;Snapshot:/Users/parssajashnieh/Zotero/storage/JJTSHGLB/1910.html:text/html},
}

@misc{zhongSeq2SQLGeneratingStructured2017,
	title = {{Seq2SQL}: {Generating} {Structured} {Queries} from {Natural} {Language} using {Reinforcement} {Learning}},
	shorttitle = {{Seq2SQL}},
	url = {http://arxiv.org/abs/1709.00103},
	doi = {10.48550/arXiv.1709.00103},
	abstract = {A significant amount of the world's knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9\% to 59.4\% and logical form accuracy from 23.4\% to 48.3\%.},
	urldate = {2025-02-24},
	publisher = {arXiv},
	author = {Zhong, Victor and Xiong, Caiming and Socher, Richard},
	month = nov,
	year = {2017},
	note = {arXiv:1709.00103 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 12 pages, 5 figures},
	file = {Preprint PDF:/Users/parssajashnieh/Zotero/storage/7FIAS6ZR/Zhong et al. - 2017 - Seq2SQL Generating Structured Queries from Natura.pdf:application/pdf;Snapshot:/Users/parssajashnieh/Zotero/storage/VRYJHUUK/1709.html:text/html},
}

@misc{wangRATSQLRelationAwareSchema2021,
	title = {{RAT}-{SQL}: {Relation}-{Aware} {Schema} {Encoding} and {Linking} for {Text}-to-{SQL} {Parsers}},
	shorttitle = {{RAT}-{SQL}},
	url = {http://arxiv.org/abs/1911.04942},
	doi = {10.48550/arXiv.1911.04942},
	abstract = {When translating natural language questions into SQL queries to answer questions from a database, contemporary semantic parsing models struggle to generalize to unseen database schemas. The generalization challenge lies in (a) encoding the database relations in an accessible way for the semantic parser, and (b) modeling alignment between database columns and their mentions in a given query. We present a unified framework, based on the relation-aware self-attention mechanism, to address schema encoding, schema linking, and feature representation within a text-to-SQL encoder. On the challenging Spider dataset this framework boosts the exact match accuracy to 57.2\%, surpassing its best counterparts by 8.7\% absolute improvement. Further augmented with BERT, it achieves the new state-of-the-art performance of 65.6\% on the Spider leaderboard. In addition, we observe qualitative improvements in the model's understanding of schema linking and alignment. Our implementation will be open-sourced at https://github.com/Microsoft/rat-sql.},
	urldate = {2025-02-24},
	publisher = {arXiv},
	author = {Wang, Bailin and Shin, Richard and Liu, Xiaodong and Polozov, Oleksandr and Richardson, Matthew},
	month = aug,
	year = {2021},
	note = {arXiv:1911.04942 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Fix some errors of ACL 2020 camera-ready; 12 pages, 5 figures, 7 tables. arXiv admin note: text overlap with arXiv:1906.11790},
	file = {Preprint PDF:/Users/parssajashnieh/Zotero/storage/7US3TBT3/Wang et al. - 2021 - RAT-SQL Relation-Aware Schema Encoding and Linkin.pdf:application/pdf;Snapshot:/Users/parssajashnieh/Zotero/storage/APNSVL6Y/1911.html:text/html},
}

@misc{GenerateSQLQueries,
  title = {Generate {SQL} {Queries} in {Seconds} for {Free} - {SQLAI}.ai},
  url = {https://www.sqlai.ai},
  abstract = {Effortlessly generate, optimize, fix, simplify, and explain SQL queries with innovative step-by-step approach and top-tier AI.},
  language = {en},
  year = {2023},
  urldate = {2025-02-24},
  file = {Snapshot:/Users/parssajashnieh/Zotero/storage/PJLJPXNW/www.sqlai.ai.html:text/html},
}

@misc{edogsHundemarktHundeOnline,
	title = {Hundemarkt: {Hunde} online finden - edogs.de},
	shorttitle = {Hundemarkt},
	year = {2017},
	url = {https://www.edogs.de/},
	abstract = {Deutschlands sicherste Hundevermittlung edogs ► Hunde finden ✓ Einfach und unkompliziert ➽ Jetzt Traumhund finden auf edogs.de!},
	language = {de},
	urldate = {2025-02-26},
	author = {edogs},
	file = {Snapshot:/Users/parssajashnieh/Zotero/storage/6TJHTZR4/www.edogs.de.html:text/html},
}

@misc{MetallamaLlama3370BInstructHugging2024,
	title = {meta-llama/{Llama}-3.3-{70B}-{Instruct} · {Hugging} {Face}},
	url = {https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2025-02-26},
	month = dec,
	year = {2024},
	file = {Snapshot:/Users/parssajashnieh/Zotero/storage/JK6DYQC5/Llama-3.html:text/html},
}

@misc{MicrosoftPhi4Hugging2025,
	title = {microsoft/phi-4 · {Hugging} {Face}},
	url = {https://huggingface.co/microsoft/phi-4},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2025-02-26},
	month = jan,
	year = {2025},
	file = {Snapshot:/Users/parssajashnieh/Zotero/storage/TLGV4MGS/phi-4.html:text/html},
}

@article{soboEvaluatingLLMsCode2025,
	title = {Evaluating {LLMs} for {Code} {Generation} in {HRI}: {A} {Comparative} {Study} of {ChatGPT}, {Gemini}, and {Claude}},
	volume = {39},
	issn = {0883-9514, 1087-6545},
	shorttitle = {Evaluating {LLMs} for {Code} {Generation} in {HRI}},
	url = {https://www.tandfonline.com/doi/full/10.1080/08839514.2024.2439610},
	doi = {10.1080/08839514.2024.2439610},
	language = {en},
	number = {1},
	urldate = {2025-02-26},
	journal = {Applied Artificial Intelligence},
	author = {Sobo, Andrei and Mubarak, Awes and Baimagambetov, Almas and Polatidis, Nikolaos},
	month = dec,
	year = {2025},
	pages = {2439610},
	file = {Full Text PDF:/Users/parssajashnieh/Zotero/storage/H6W6ZZ42/Sobo et al. - 2025 - Evaluating LLMs for Code Generation in HRI A Comp.pdf:application/pdf},
}

@misc{gaoTextSQLEmpoweredLarge2023,
	title = {Text-to-{SQL} {Empowered} by {Large} {Language} {Models}: {A} {Benchmark} {Evaluation}},
	shorttitle = {Text-to-{SQL} {Empowered} by {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2308.15363},
	doi = {10.48550/arXiv.2308.15363},
	abstract = {Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL task. However, the absence of a systematical benchmark inhibits the development of designing effective, efficient and economic LLM-based Text-to-SQL solutions. To address this challenge, in this paper, we first conduct a systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, and with these experimental results, we elaborate their pros and cons. Based on these findings, we propose a new integrated solution, named DAIL-SQL, which refreshes the Spider leaderboard with 86.6\% execution accuracy and sets a new bar. To explore the potential of open-source LLM, we investigate them in various scenarios, and further enhance their performance with supervised fine-tuning. Our explorations highlight open-source LLMs' potential in Text-to-SQL, as well as the advantages and disadvantages of the supervised fine-tuning. Additionally, towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize the token efficiency in prompt engineering and compare the prior studies under this metric. We hope that our work provides a deeper understanding of Text-to-SQL with LLMs, and inspires further investigations and broad applications.},
	urldate = {2025-02-27},
	publisher = {arXiv},
	author = {Gao, Dawei and Wang, Haibin and Li, Yaliang and Sun, Xiuyu and Qian, Yichen and Ding, Bolin and Zhou, Jingren},
	month = nov,
	year = {2023},
	note = {arXiv:2308.15363 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Databases, Computer Science - Machine Learning},
	annote = {Comment: We have released code on https://github.com/BeachWang/DAIL-SQL},
	file = {Preprint PDF:/Users/parssajashnieh/Zotero/storage/JYMIF94G/Gao et al. - 2023 - Text-to-SQL Empowered by Large Language Models A .pdf:application/pdf;Snapshot:/Users/parssajashnieh/Zotero/storage/RUSHKYTK/2308.html:text/html},
}

@misc{jashniehParssaJHCIAI2025,
	title = {{ParssaJ}/{HCI}-{AI}},
	url = {https://github.com/ParssaJ/HCI-AI},
	abstract = {Wir stellen unser eingereichtes Projekt samt Paper + Präsentationen für das HCI Modul für interessierte ein. Schaut es euch gerne an!},
	urldate = {2025-02-27},
	author = {Jashnieh, Parssa},
	month = feb,
	year = {2025},
	note = {original-date: 2024-10-09T15:55:13Z},
}
